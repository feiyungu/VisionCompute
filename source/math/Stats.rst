统计推倒
***********


`Normal inverse Wishart distribution <http://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution>`_
==========================================================================================================

它是是多变量正态分布的共轭先验。

假定：

.. math::

   \mu|\mu_0,\lambda ,\Sigma \sim N(\mu|\mu_0,\frac{1}{\lambda}\Sigma)

假设存在一个多变量正态分布，期望为 :math:`\mu_0`, 方差为 :math:`\frac{1}{\lambda}\Sigma` ， 其中

.. math::

   \Sigma|\Phi,v\sim N(\Sigma|\Phi, v)

服从inverse Wishart distribution，那么 :math:`(\mu,\Sigma)` 的联合概率密度函数服从:

.. math::
 
   (\mu,\Sigma)\sim NIW(\mu_0,\lambda,\Phi,v)

这里 :math:`\Phi \in R^{D\times D}` 是一个逆尺度矩阵。

`Gamma分布 <http://www.52nlp.cn/lda-math-%E7%A5%9E%E5%A5%87%E7%9A%84gamma%E5%87%BD%E6%95%B03>`_ 
==================================================================================================

Gamma 分布在概率统计中频繁现身，众多的统计分布，包括常见的统计学三大分布(t分布， :math:`\chi^2`, F分布)、Beta分布、Dirichlet分布的密度公式中都有Gamma的身影，当然最直接的概率分布是直接由Gamma函数变换得到的Gamma分布。对Gamma 函数的定义做一个变形，就得到如下式子：

.. math::

   \int_0^\infty \frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)}dx =1

取积分中的函数作为概率密度，就得到形式简单的Gamma分布的概率密度.

.. math::

   Gamma(x|\alpha)=\frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)}

把 :math:`x= \beta t` 代入，得到：

.. math::

   Gamma(t|\alpha,\beta)=\frac{\beta^\alpha  t^{(\alpha-1)}  e^{(-\beta t)} }{\Gamma(\alpha)}


其中 :math:`\alpha` 称为 shape parameter， 主要决定了分布曲线的形状， 而 :math:`\beta` 称为 rate parameter 或者 inverse scale parameter ( :math:`\frac{1}{\beta}` 称为 scale parameter)， 主要决定曲线有多陡。

Gamma 分布的迷人之处
-----------------------

Gamma 分布与Poisson 分布、 Poisson 过程发生这密切的关系。参数为 :math:`\lambda` 的Poisson分布概率可以表示为：

.. math::

   Poisson(X=k|\lambda) = \frac{\lambda^ke^{-\lambda}}{k!}

在Gamma分布的密度中取 :math:`\alpha = k+1` 得到：

.. math::

   Gamma(x|\alpha= k+1)=\frac{x^ke^{-x}}{\Gamma(k+1)} = \frac{x^ke^{-x}}{k!}

`t分布 <http://class.htu.cn/jingjiyingyongtongji/seven5.htm>`_
---------------------------------------------------------------


t分布是从正态分布而来的，但是在实际工作中， :math:`\sigma` 往往是未知的，常用s作为 :math:`\sigma` 的估计值，为了与u变换做区别，称为t变换，称为统计t值的分布称为t分布。

从均值为L,方差为 :math:`R^2` 的正态总体中抽取容量为n的一个样本，其样本平均数 :math:`\bar x` 服从均值为L、方差为 :math:`R^2/n` 的正态分布，因此：

.. math::

   \frac{\bar x -\mu}{s/\sqrt{n}}  \sim N(0,1)

但是总体方差  :math:`R^2/n` 是未知的，只能使用 :math:`s^2/n`: 代替，如果n很大， :math:`s^2/n`: 就是  :math:`s^2/n`: 的一个较好的估计量， :math:`\frac{\bar x -\mu}{s/\sqrt{n}}` 仍然是一个标准的正态分布；

如果n较小， :math:`s^2/n` 与  :math:`s^2/n` 的差异较大，因此统计量 :math:`\frac{\bar x -\mu}{s/\sqrt{n}}` 就不再是一个标准正态分布，而是服从t分布。


t分布式检验一个样本平均数与一个已知的总体平均数的差异是否明显。t分布检验统计量为：


.. math::
 
   t\ frac{\bar X \mu}{\frac{\sigma}{\sqrt{n1}}}

.. note:

   描述一个局部样本和正义样本的差异统计量。

See also
========

#. `随​机​过​程​-​简​版 <http://wenku.baidu.com/view/0cae7d4ce518964bcf847c48.html>`_  用打电话把随机过程给讲明白了
#. `随​机​过​程 <http://wenku.baidu.com/view/5ceb8a59804d2b160b4ec0cc.html>`_  定义讲的不错

Thinking
========

*随机从何而来因果论* 如果是随机就是否定因果论。其实二者是统一的，随机就是还没有发现的因果关系。为什么宙宇飞船都精确的控制，而一场球赛却不能知道结果呢，这个过程有什么难度，区别就在于飞船的每一个动作都可以精确的控制，而足球做不到精确的控制。

人们做错了事情的时候，都会找到原因，而不会归因于随机吧。 人能够走多远，取决于你明天要做什么，如果已经知道你明天要做什么，就会知道明天能走多远。首先要知道你明天要干做什么。然后才能决定明天能够走多远。

先验概率与后验概率区别在哪里，就在于未知因素减少。如果未知因素没有改变，就会存在先验与后验的问题。这也就解决了为什么大部分人炒股会赔的原因。那就是随机论来说的，那些专家更容易赚呢，为什么庄家赚的最多，源于他的未知因素最少。当然庄家也会有未知因素，例如大环境等等，所以他们也可能会赔。如果想赚钱就要解决这些问题。解决这些问题你就胜券在握了。

就像金属制作一样，现在我们掌握原理就是一个必然的东东，而在古代不知道的情况下那就只能自然条件来得到了。
所以遇到一个事情，成功胜算，就在于对于未知因素的多与少。如果不能知道，就要能快速反应。


*自相关与协方差为什么是卷积* 卷积本质就是循环遍历，例如每组元素之间的关系影响是怎么的，其本质就一个简单的干法每两个元素之间做乘加运算。然后这个值来判断两组数据之间的相关性。加乘其实就是线性运算。如果这两个运算给换掉呢，例如用指数加乘计算的结果会是怎么样的。所谓的分类两组数据之间的相关性。其实也就是相关系数的计算方法的问题。用线性运算的卷积。如果采用非线性卷积呢。

卷积的本质就是要做穷举的加乘运算。

伯努利大数定律是契比谢夫不等式的简单推论。`契比谢夫不等式是可以用来预测当期望与方差知道的情况下 <http://doc.mbalib.com/view/7889f2cb10485a7e004cc8d1ed9bda79.html>`_ 。

*等式与不等式之间* 转换用极限以及KTT条件


伯努利在结束<推测术>时就其结果的意义 作了如下的表述：如果我们能把一切事件永恒地观察下去，则我们终将发现，世间的一切事物都受到因果律的支配，而我们注定会在种种极其纷纭杂乱的事象中认识到某种必然。

伯努利在趋近于极限的情况下，也就是求极限的时候就是正态了。二项分布的期望与方差，并且与次数之间的关系是什么。
正态分布就是方差的分布图，只是做一个转换而己其本质就在
.. math:: f(x)=(x-\mu)^2/\sigma^2
二项分布期望以及方差以及变化情况，以及期望与方差的比值是怎么样的, 各种分布在解决什么问题，那就是概率与统计之间的关系。所谓的各种分布就是为解决统计与概率之间关系。正态分布其本质就是方差分布的变形而己。


*用频率含估计概率的精度* ，大致上是与试验次数N的平方根成比例的。这个要用到极限以及收敛速度的问题。就像用无穷极数要保证计算的精度的问题一样，当然极数越短计算越简单。但是要根据误差取得这个值。见 P43 快盘/math/陈希孺-数理统计学简史.pdf   这个也就解决确定性不可知因数控制关系了。对于非常复杂的计算，能否用无穷极数来简化计算，但是又需要多少极，来保证达到要求的精度。也就是PCA算法，到底留多少主分量呢。

误差分析与收敛速度。  这个也就是为什么极限经常要那个不等式无穷小来得到在N的意义了。


*阶乘的级数计算* 会用到
.. math:: \pi `斯特靈公式 <http://zh.wikipedia.org/wiki/%E6%96%AF%E7%89%B9%E9%9D%88%E5%85%AC%E5%BC%8F>`_ 


* 最小二乘* 
.. math:: targetFunction=\Sigma(obversation- thoeryValue)^2$% 另一个那就是解线矛盾方程。但是最小二乘稳定性不好，换成一个通用写法
.. math:: M(\theta)=\sum_{i=1}^{n} \rho(x_{i}-\theta)而%$\rho可以根据自己的需要去换掉，二次就是最小二乘，也可以是一次或者直接最小值。


*先验分布+样本信息=后验分布*
