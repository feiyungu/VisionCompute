视觉里程计算
============

原理是通过分析处理相关图像序列来确定的机器人的位置和姿态。 就像汽车里里程表一样。本质就是物体在图像空间的运动，反算出在真实空间运动。

其实就是空间域的变换， 在一个空间来进行另一个空间的计算。 就像FFT的频域分析，通过时频转换，通过测量频域我们通过测量频域测量时域。

所以我们真的需要，每一步都进行一步转换，还是只需要在开头与结尾进行一次的转换。
关键是找到域的转换关系，以及各个域的映射关系。

例如真实世界中刚体运动与相机坐标中什么运动是一一映射的。 如果是这样的。例如先图像拼接成一张大图，然后直接计算物体在图像坐标的运动。 最后在反算回去。

每一个测量技术都构成了一个技术域，我们的目标是从要找到技术域与目标域的路径，映射转换关系。 
位置x,y,z,姿态 roll,pitch,yaw.

http://www.fengbing.net/2015/07/25/%E8%A7%86%E8%A7%89%E9%87%8C%E7%A8%8B%E8%AE%A1%E7%AE%80%E4%BB%8B/



这个过程发展过程是这样里程计，odometry,推算定位dead reckoning，到最后视觉计算。

里程计，也就是速度对时间积分，主要通过运动机置的编码器为基础来计算。
dead reckoning,借助于先前己知位置，以及估算出的速度随时间变化量。
实质是一个递推计算问题，现在流行IMU 惯性导航系统。




基本过程
========

#. 获取图像 It,It+1.
#. 对获得图像进行畸变处理
#. 特征检测，跟踪图像序列中特片。
#. 估算图像本质矩阵,估计R,t.
#. 对尺度信息进行估计，最终确定旋转矩阵和平移向量

因为解方程常常是没有冗余方程组，并且得到最优解。一般是最小二乘以及SVD，LU等等。是以照片的张数来而迭代的。
理论上两张图就够了，如果标定已经对象，一个张图就够了。 关键是方程的个数。
实际测下来20张左右。也就是精度是最优的。对于这种优化的迭代，就需要知道迭代多少度来得足够的精度。一般情况下
迭代的次数是与某些参数相关的。每一个照片对应一个平面。

单目视觉里里程计算
双目视觉里程计算

VIO - Visual Inertial Odometry
VIO和之前的几种SLAM最大的不同在于两点：首先，VIO在硬件上需要传感器的融合，包括相机和六轴陀螺仪，相机产生图片，六轴陀螺仪产生加速度和角速度。相机相对准但相对慢，六轴陀螺仪的原始加速度如果拿来直接积分会在很短的时间飘走（zero-drift），但六轴陀螺仪的频率很高，在手机上都有200Hz。其次，VIO实现的是一种比较复杂而有效的卡尔曼滤波，比如MSCKF（Multi-State-Constraint-Kalman-Filter），侧重的是快速的姿态跟踪，而不花精力来维护全局地图，也不做keyframe based SLAM里面的针对地图的全局优化（bundle adjustment）。最著名的商业化实现就是Google的Project Tango和已经被苹果收购的Flyby Media，其中第二代Project Tango搭载了Nividia TK1并有主动光源的深度摄像头的平板电脑，这款硬件可谓每个做算法的小伙伴的梦幻搭档，具体在这里不多阐述。



Bundle Adjustment 光束法平差详解
================================ 

本质上就是Ax = B 最优解的问题，基本上都是最小二乘，基本是Gauss-Newton或者LM算法迭代求解。本质就是LM算是，它把误差"平均" 分配到每个观测值上，以避免误差积累导致的结果比前面的差很多。

历史的原因，平差始于大地测量技术，测量中最基础的就是三角测量，大地测量或摄影测量都是基于此，观测数据上规则，有冗余的情况下，必然牵扯平差，提高测量结果的精度和可靠度。 google的地球街景就用这个来搞的。

http://blog.csdn.net/junshen1314/article/details/48860951 这个不正是自己所想传感器融合的方法。每一个点被多个传感器同时检测到，得到一个这些检测误差的值就是真值。 

数值计算到后期玩的就都是误差分析。
http://blog.csdn.net/abcjennifer/article/details/7588865
